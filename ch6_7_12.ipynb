{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9af4e738",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Chris', 'NNP'), ('loved', 'VBD'), ('outdoor', 'RP'), ('running', 'VBG')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#6.7 Tagging Parts of Speech\n",
    "\n",
    "# Load libraries\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Create text\n",
    "text_data = \"Chris loved outdoor running\"\n",
    "\n",
    "# Use pretrained part of speech tagger\n",
    "text_tagged = pos_tag(word_tokenize(text_data))\n",
    "\n",
    "# Show parts of speech\n",
    "text_tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "70c9260b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Chris']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter words\n",
    "[word for word, tag in text_tagged if tag in ['NN', 'NNS', 'NNP', 'NNPS']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46304812",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 0, 1, 0, 1, 1, 1, 0],\n",
       "       [1, 0, 1, 1, 0, 0, 0, 0, 1],\n",
       "       [1, 0, 1, 1, 1, 0, 0, 0, 1]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import libraries\n",
    "import nltk\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "# Create text\n",
    "tweets = [\"I am eating a burrito for breakfast\",\n",
    "\"Political science is an amazing field\",\n",
    "\"San Francisco is an awesome city\"]\n",
    "\n",
    "# Create list\n",
    "tagged_tweets = []\n",
    "\n",
    "# Tag each word and each tweet\n",
    "for tweet in tweets:\n",
    "    tweet_tag = nltk.pos_tag(word_tokenize(tweet))\n",
    "    tagged_tweets.append([tag for word, tag in tweet_tag])\n",
    "    \n",
    "# Use one-hot encoding to convert the tags into features\n",
    "one_hot_multi = MultiLabelBinarizer()\n",
    "one_hot_multi.fit_transform(tagged_tweets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80a77f30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Elon Musk, Twitter, 21B)\n",
      "- - - - - - - - - - - - -\n",
      "Elon Musk,PERSON\n",
      "Twitter,PERSON\n",
      "21B,MONEY\n"
     ]
    }
   ],
   "source": [
    "#6.8 Performing Named-Entity Recognition\n",
    "\n",
    "# Import libraries\n",
    "import spacy\n",
    "\n",
    "# Load the spaCy package and use it to parse the text\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Elon Musk offered to buy Twitter using $21B of his own money.\")\n",
    "\n",
    "# Print each entity\n",
    "print(doc.ents)\n",
    "print(\"- - - - - - - - - - - - -\")\n",
    "\n",
    "# For each entity print the text and the entity label\n",
    "for entity in doc.ents:\n",
    "    print(entity.text, entity.label_, sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7653729",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Compressed Sparse Row sparse matrix of dtype 'int64'\n",
       "\twith 8 stored elements and shape (3, 8)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#6.9 Encoding Text as a Bag of Words\n",
    "\n",
    "# Load library\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Create text\n",
    "text_data = np.array(['I love Brazil. I Brazil!',\n",
    "                       'Sweden is best',\n",
    "                       'Germany beats both'])\n",
    "\n",
    "# Create the bag of words feature matrix\n",
    "count = CountVectorizer()\n",
    "bag_of_words = count.fit_transform(text_data)\n",
    "\n",
    "# Show feature matrix\n",
    "bag_of_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67730953",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 2, 0, 0, 1, 0],\n",
       "       [0, 1, 0, 0, 0, 1, 0, 1],\n",
       "       [1, 0, 1, 0, 1, 0, 0, 0]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_of_words.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bed8d239",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['beats', 'best', 'both', 'brazil', 'germany', 'is', 'love',\n",
       "       'sweden'], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show feature names\n",
    "count.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62dcec8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 1]\n",
      " [0 0]\n",
      " [0 0]]\n",
      "- - - - - - - - -\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'brazil': 0, 'love': 1}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create feature matrix with arguments\n",
    "count_2gram = CountVectorizer(ngram_range=(1, 2),\n",
    "                              stop_words=\"english\",\n",
    "                              vocabulary=[\"brazil\", \"love\"])\n",
    "\n",
    "bag = count_2gram.fit_transform(text_data)\n",
    "\n",
    "# View feature matrix\n",
    "print(bag.toarray())\n",
    "print(\"- - - - - - - - -\")\n",
    "# View the 1-grams and 2-grams\n",
    "count_2gram.vocabulary_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18af40a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
       "\twith 8 stored elements and shape (3, 8)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#6.10 Weighting Word Importance\n",
    "\n",
    "# Load libraries\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#Create text\n",
    "text_data = np.array(['I love Brazil. Brazil!',\n",
    "                      'Sweden is best',\n",
    "                      'Germany beats both'])\n",
    "\n",
    "# Create the tf-idf feature matrix\n",
    "tfidf = TfidfVectorizer()\n",
    "feature_matrix = tfidf.fit_transform(text_data)\n",
    "\n",
    "# Show tf-idf feature matrix\n",
    "feature_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eef0f87b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.89442719, 0.        ,\n",
       "        0.        , 0.4472136 , 0.        ],\n",
       "       [0.        , 0.57735027, 0.        , 0.        , 0.        ,\n",
       "        0.57735027, 0.        , 0.57735027],\n",
       "       [0.57735027, 0.        , 0.57735027, 0.        , 0.57735027,\n",
       "        0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show tf-idf feature matrix as dense matrix\n",
    "feature_matrix.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5aa17e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'love': 6,\n",
       " 'brazil': 3,\n",
       " 'sweden': 7,\n",
       " 'is': 5,\n",
       " 'best': 1,\n",
       " 'germany': 4,\n",
       " 'beats': 0,\n",
       " 'both': 2}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show feature names\n",
    "tfidf.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "70dc29a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(np.str_('Sweden is best'), np.float64(0.6666666666666666)), (np.str_('I love Brazil. Brazil!'), np.float64(0.5163977794943222)), (np.str_('Germany beats both'), np.float64(0.0))]\n"
     ]
    }
   ],
   "source": [
    "#6.11 Using Text Vectors to Calculate Text Similarity in a Search Query\n",
    "\n",
    "# Load libraries\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "# Create searchable text data\n",
    "text_data = np.array(['I love Brazil. Brazil!',\n",
    "                      'Sweden is best',\n",
    "                      'Germany beats both'])\n",
    "\n",
    "# Create the tf-idf feature matrix\n",
    "tdidf = TfidfVectorizer()\n",
    "feature_matrix = tdidf.fit_transform(text_data)\n",
    "\n",
    "# Create a search query and transform it into a tf-idf vector\n",
    "text = \"Brazil is the best\"\n",
    "vector = tdidf.transform([text])\n",
    "\n",
    "# Calculate the cosine similarities between the input vector and all other vectors\n",
    "cosine_simialrities = linear_kernel(vector,feature_matrix).flatten()\n",
    "\n",
    "# Get the index of the most relevent items in order\n",
    "related_doc_indicies = cosine_simialrities.argsort()[::-1]\n",
    "\n",
    "# Print the most similar texts to the search query along with the cosine similarity\n",
    "print([(text_data[i], cosine_simialrities[i]) for i in related_doc_indicies])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a42352b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'NEGATIVE', 'score': 0.9998020529747009}] [{'label': 'POSITIVE', 'score': 0.9995730519294739}]\n"
     ]
    }
   ],
   "source": [
    "#6.12 Using a Sentiment Analysis Classifier\n",
    "\n",
    "# Import libraries\n",
    "from transformers import pipeline\n",
    "\n",
    "# Create an NLP pipeline that runs sentiment analysis\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "# Classify some text\n",
    "sentiment_1 = classifier(\"I hate machine learning! It's the absolute worst.\")\n",
    "sentiment_2 = classifier(\n",
    "    \"Machine learning is the absolute\"\n",
    "    \"bees knees I love it so much!\"\n",
    ")\n",
    "\n",
    "# Print sentiment output\n",
    "print(sentiment_1, sentiment_2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
